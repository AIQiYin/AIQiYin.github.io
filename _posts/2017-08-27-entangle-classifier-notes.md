---
layout: post
title: "Entangle Classifier Notes"
description: ""
category: tech
tags: []
modify: 2017-08-27 19:45:19
---

update: 2017-08-27


1. 网络第一隐藏层若是relu单元，节点个数要大于输入节点以防止降维而损失有用信息。简单来看，relu单元接近线性变换，如果节点变少就是投影到一个更小的空间从而损失某些信息，除非目标函数中有简单的线性函数。当然，relu的截断特性又可以使得更多的单元创造出更多的维度空间，类似于高次项的作用。鉴于此，为了模型的表达能力，简单点，在一层内最好有不同的激活函数以避免信息在层间传递的丢失。

2. 第二层，对于sigmoid之类的函数，如泰勒展开的无穷级数式加权和来拟合任意函数，在该类层中应当选择足够的单元来减少高阶项即残差。但由于其是有界的，原则上不能表达无界函数，所以同样应增加其他激活函数，如增加其逆函数最好，这不必在同一层。

3. 以上对于深层网络、cnn、有特点的目标函数不能简单运用，还需具体分析。不过，sigmoid函数在深层网络中的多次前向迭代使得有界的弱点不断突出，这可能和backward的误差梯度消失相呼应。又如，cnn采用部分卷积共享参数而没有全链接，这里也没有减小空间，不考虑relu的因素，因为无论全链接还是部分卷积都是线性变换，变换之后的空间有效维度都是原来那么多，与其增加参数个数，不如增加节点和relu激活函数来提高表达能力。relu的表达能力即分段函数，大大不同于简单的线性函数。

4. relu的分段能力类似决策树但又不全像，决策树模拟人的思考过程，但太过绝对也不好数值确定分割点。或许可以用一种类似relu的分段方式对决策树进行改造。
